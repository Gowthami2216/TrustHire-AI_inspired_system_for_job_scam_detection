{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustHire â€“ AI-Powered Job Scam Detection System\n",
    "\n",
    "## Machine Learning Pipeline for Detecting Fake Job Offers\n",
    "\n",
    "This notebook implements a complete ML pipeline for:\n",
    "1. **Text Classification** - Classify job offers as Scam/Genuine\n",
    "2. **NLP Feature Extraction** - Extract keywords, sentiment, and red flags\n",
    "3. **Image OCR + Analysis** - Extract and analyze text from screenshots\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install pandas numpy scikit-learn nltk spacy pytesseract pillow transformers torch joblib matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# OCR\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "We'll create a synthetic dataset of job postings labeled as scam or genuine.\n",
    "In production, you would use real labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic dataset for training (In production, use real labeled data)\n",
    "# Label: 1 = Scam, 0 = Genuine\n",
    "\n",
    "scam_examples = [\n",
    "    \"Congratulations! You have been selected for a high-paying job. Pay $200 registration fee to proceed.\",\n",
    "    \"Urgent hiring! Work from home and earn $5000/week. No experience needed. Send bank details.\",\n",
    "    \"Dear candidate, you are shortlisted. Transfer Rs 5000 for training materials. Contact on WhatsApp.\",\n",
    "    \"Immediate joining! $100/hour data entry job. Pay security deposit of $500.\",\n",
    "    \"You won our job lottery! Send personal documents and bank account to claim position.\",\n",
    "    \"Limited time offer! High salary job guaranteed. Registration fee required. Act now!\",\n",
    "    \"Hiring for multinational company. 50K salary. Pay processing fee. WhatsApp only.\",\n",
    "    \"Work from home earning $10000 monthly. Just need your SSN and bank routing number.\",\n",
    "    \"Congratulation! Selected for interview. Pay $100 for background verification.\",\n",
    "    \"Job offer from Google! Salary $200k. Send money for visa processing.\",\n",
    "    \"Urgent requirement! Pay Rs 2000 for id card and uniform. Join tomorrow!\",\n",
    "    \"You are hired! Send copy of passport and $300 for equipment deposit.\",\n",
    "    \"Dream job opportunity! No interview needed. Just pay joining fee of $150.\",\n",
    "    \"Selected for Amazon position. Pay $400 for training. Contact: scammer@gmail.com\",\n",
    "    \"High paying job! $500/day guaranteed. Send bank details for salary setup.\",\n",
    "    \"Congratulations on selection! Pay advance salary tax of Rs 10000.\",\n",
    "    \"Work from home data entry. $50/hour. Pay $200 for software license.\",\n",
    "    \"Immediate hiring! Send OTP received on your phone to verify account.\",\n",
    "    \"Job confirmed at Microsoft! Transfer $600 for work visa.\",\n",
    "    \"Freelance opportunity! $1000/week. Need your credit card for verification.\"\n",
    "]\n",
    "\n",
    "genuine_examples = [\n",
    "    \"We are pleased to offer you the position of Software Engineer at our company with annual salary of $85,000.\",\n",
    "    \"Thank you for interviewing. We would like to invite you for a second round at our office.\",\n",
    "    \"Your application for Data Analyst has been received. HR will contact you within 5 business days.\",\n",
    "    \"We are looking for experienced Python developers. Apply through our careers portal.\",\n",
    "    \"Interview scheduled for Monday 10 AM at our headquarters. Please bring your resume and ID.\",\n",
    "    \"Position: Marketing Manager. Salary: $70,000 + benefits. Apply at careers.company.com\",\n",
    "    \"Thank you for applying. Your resume has been forwarded to the hiring manager.\",\n",
    "    \"We are hiring interns for our summer program. Stipend: $1500/month. Apply online.\",\n",
    "    \"Job offer attached. Please review the terms and sign the acceptance letter.\",\n",
    "    \"Congratulations on passing the technical interview. Final HR round scheduled for Friday.\",\n",
    "    \"We require 3+ years experience in React.js. Competitive salary and remote options.\",\n",
    "    \"Your background check is complete. Onboarding documents will be sent via official email.\",\n",
    "    \"Position filled. Thank you for your interest. We will keep your resume on file.\",\n",
    "    \"Salary negotiation meeting scheduled. Please confirm your availability.\",\n",
    "    \"Job opening: Senior Developer at ABC Corp. Apply through LinkedIn or company website.\",\n",
    "    \"HR department will send you the offer letter. No fees or deposits required.\",\n",
    "    \"Welcome aboard! Your first day is March 15. Report to HR for orientation.\",\n",
    "    \"We offer health insurance, 401k, and 20 days PTO. Review benefits package attached.\",\n",
    "    \"Technical assessment results: Passed. Next step is the team fit interview.\",\n",
    "    \"Reference check completed successfully. Offer letter will be emailed today.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'text': scam_examples + genuine_examples,\n",
    "    'label': [1] * len(scam_examples) + [0] * len(genuine_examples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Text preprocessing pipeline for job scam detection.\n",
    "    Handles cleaning, tokenization, and lemmatization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Keep important negation words\n",
    "        self.stop_words -= {'no', 'not', 'don', \"don't\", 'won', \"won't\", 'shouldn', \"shouldn't\"}\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Basic text cleaning\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses but keep a marker\n",
    "        text = re.sub(r'\\S+@\\S+', ' EMAIL_ADDRESS ', text)\n",
    "        \n",
    "        # Detect money amounts\n",
    "        text = re.sub(r'\\$[\\d,]+|rs\\.?\\s*[\\d,]+|â‚¹[\\d,]+', ' MONEY_AMOUNT ', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove special characters but keep spaces\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize_and_lemmatize(self, text):\n",
    "        \"\"\"Tokenize and lemmatize text\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                  if token not in self.stop_words and len(token) > 2]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Full preprocessing pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        processed = self.tokenize_and_lemmatize(cleaned)\n",
    "        return processed\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Apply preprocessing to dataset\n",
    "df['processed_text'] = df['text'].apply(preprocessor.preprocess)\n",
    "\n",
    "print(\"Sample preprocessed texts:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df.iloc[i]['text'][:80]}...\")\n",
    "    print(f\"Processed: {df.iloc[i]['processed_text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Scam Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScamFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract scam-specific features from job postings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Scam indicator keywords\n",
    "        self.payment_words = {'pay', 'fee', 'deposit', 'transfer', 'money', 'payment', \n",
    "                              'registration', 'processing', 'advance', 'cost', 'charge'}\n",
    "        self.urgency_words = {'urgent', 'immediately', 'now', 'today', 'asap', 'hurry',\n",
    "                              'limited', 'act', 'quick', 'fast', 'deadline'}\n",
    "        self.too_good_words = {'guaranteed', 'easy', 'simple', 'no experience', 'dream',\n",
    "                               'amazing', 'incredible', 'lottery', 'won', 'selected'}\n",
    "        self.personal_info_words = {'bank', 'account', 'ssn', 'passport', 'id', 'credit card',\n",
    "                                    'routing', 'password', 'otp', 'verification'}\n",
    "        self.suspicious_contact = {'whatsapp', 'telegram', 'gmail', 'yahoo', 'hotmail'}\n",
    "    \n",
    "    def count_keyword_matches(self, text, keywords):\n",
    "        \"\"\"Count how many keywords from a set appear in text\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        return sum(1 for word in keywords if word in text_lower)\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract all scam-related features from text\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        features = {\n",
    "            # Keyword-based features\n",
    "            'payment_mentions': self.count_keyword_matches(text, self.payment_words),\n",
    "            'urgency_mentions': self.count_keyword_matches(text, self.urgency_words),\n",
    "            'too_good_mentions': self.count_keyword_matches(text, self.too_good_words),\n",
    "            'personal_info_requests': self.count_keyword_matches(text, self.personal_info_words),\n",
    "            'suspicious_contact': self.count_keyword_matches(text, self.suspicious_contact),\n",
    "            \n",
    "            # Pattern-based features\n",
    "            'has_money_amount': 1 if re.search(r'\\$[\\d,]+|rs\\.?\\s*[\\d,]+|â‚¹[\\d,]+', text_lower) else 0,\n",
    "            'exclamation_count': text.count('!'),\n",
    "            'caps_ratio': sum(1 for c in text if c.isupper()) / max(len(text), 1),\n",
    "            'word_count': len(text.split()),\n",
    "            \n",
    "            # Sentiment features\n",
    "            'sentiment_compound': self.sia.polarity_scores(text)['compound'],\n",
    "            'sentiment_positive': self.sia.polarity_scores(text)['pos'],\n",
    "            'sentiment_negative': self.sia.polarity_scores(text)['neg'],\n",
    "            \n",
    "            # Red flag combinations\n",
    "            'payment_urgency_combo': 1 if (self.count_keyword_matches(text, self.payment_words) > 0 \n",
    "                                           and self.count_keyword_matches(text, self.urgency_words) > 0) else 0,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def get_red_flags(self, text):\n",
    "        \"\"\"Get list of detected red flags in text\"\"\"\n",
    "        red_flags = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if self.count_keyword_matches(text, self.payment_words) > 0:\n",
    "            red_flags.append(\"Payment or fee requested\")\n",
    "        if self.count_keyword_matches(text, self.urgency_words) > 0:\n",
    "            red_flags.append(\"Urgency tactics detected\")\n",
    "        if self.count_keyword_matches(text, self.too_good_words) > 0:\n",
    "            red_flags.append(\"Unrealistic promises made\")\n",
    "        if self.count_keyword_matches(text, self.personal_info_words) > 0:\n",
    "            red_flags.append(\"Personal/banking information requested\")\n",
    "        if self.count_keyword_matches(text, self.suspicious_contact) > 0:\n",
    "            red_flags.append(\"Unprofessional contact methods\")\n",
    "        if re.search(r'\\$[\\d,]+|rs\\.?\\s*[\\d,]+', text_lower):\n",
    "            red_flags.append(\"Specific money amount mentioned\")\n",
    "        if text.count('!') >= 2:\n",
    "            red_flags.append(\"Excessive exclamation marks\")\n",
    "            \n",
    "        return red_flags\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = ScamFeatureExtractor()\n",
    "\n",
    "# Extract features for all samples\n",
    "feature_list = df['text'].apply(feature_extractor.extract_features).tolist()\n",
    "feature_df = pd.DataFrame(feature_list)\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_text = df['processed_text']\n",
    "X_features = feature_df\n",
    "y = df['label']\n",
    "\n",
    "# Split data\n",
    "X_text_train, X_text_test, X_feat_train, X_feat_test, y_train, y_test = train_test_split(\n",
    "    X_text, X_features, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_text_train)}\")\n",
    "print(f\"Test set size: {len(X_text_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=500,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=1,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "# Fit and transform training data\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_text_train)\n",
    "X_tfidf_test = tfidf_vectorizer.transform(X_text_test)\n",
    "\n",
    "# Combine TF-IDF features with custom features\n",
    "X_combined_train = np.hstack([X_tfidf_train.toarray(), X_feat_train.values])\n",
    "X_combined_test = np.hstack([X_tfidf_test.toarray(), X_feat_test.values])\n",
    "\n",
    "print(f\"TF-IDF features: {X_tfidf_train.shape[1]}\")\n",
    "print(f\"Custom features: {X_feat_train.shape[1]}\")\n",
    "print(f\"Total features: {X_combined_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models and compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train model\n",
    "    if name == 'Naive Bayes':\n",
    "        # Naive Bayes needs non-negative features\n",
    "        model.fit(X_tfidf_train, y_train)\n",
    "        y_pred = model.predict(X_tfidf_test)\n",
    "        y_prob = model.predict_proba(X_tfidf_test)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_combined_train, y_train)\n",
    "        y_pred = model.predict(X_combined_test)\n",
    "        y_prob = model.predict_proba(X_combined_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy,\n",
    "        'ROC-AUC': roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Genuine', 'Scam']))\n",
    "\n",
    "# Results comparison\n",
    "results_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "colors = sns.color_palette(\"husl\", len(results_df))\n",
    "axes[0].barh(results_df['Model'], results_df['Accuracy'], color=colors)\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xlim([0, 1])\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# ROC-AUC comparison\n",
    "axes[1].barh(results_df['Model'], results_df['ROC-AUC'], color=colors)\n",
    "axes[1].set_xlabel('ROC-AUC Score')\n",
    "axes[1].set_title('Model ROC-AUC Comparison')\n",
    "axes[1].set_xlim([0, 1])\n",
    "for i, v in enumerate(results_df['ROC-AUC']):\n",
    "    axes[1].text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best Model - Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest model\n",
    "best_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "best_model.fit(X_combined_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "feature_names = list(tfidf_vectorizer.get_feature_names_out()) + list(feature_df.columns)\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance_df.head(20)\n",
    "sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "plt.title('Top 20 Most Important Features for Scam Detection')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importance_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "y_pred_best = best_model.predict(X_combined_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Genuine', 'Scam'], \n",
    "            yticklabels=['Genuine', 'Scam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Random Forest Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. OCR - Image Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageOCR:\n",
    "    \"\"\"\n",
    "    Extract text from images (screenshots, job postings, chat messages).\n",
    "    Uses Tesseract OCR.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configure Tesseract path if needed (Windows)\n",
    "        # pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "        pass\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for better OCR results\"\"\"\n",
    "        from PIL import ImageFilter, ImageEnhance\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        if image.mode != 'L':\n",
    "            image = image.convert('L')\n",
    "        \n",
    "        # Enhance contrast\n",
    "        enhancer = ImageEnhance.Contrast(image)\n",
    "        image = enhancer.enhance(2)\n",
    "        \n",
    "        # Sharpen\n",
    "        image = image.filter(ImageFilter.SHARPEN)\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def extract_text(self, image_path):\n",
    "        \"\"\"Extract text from an image file\"\"\"\n",
    "        try:\n",
    "            image = Image.open(image_path)\n",
    "            processed = self.preprocess_image(image)\n",
    "            text = pytesseract.image_to_string(processed)\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting text: {str(e)}\"\n",
    "    \n",
    "    def extract_text_from_bytes(self, image_bytes):\n",
    "        \"\"\"Extract text from image bytes (for web uploads)\"\"\"\n",
    "        from io import BytesIO\n",
    "        try:\n",
    "            image = Image.open(BytesIO(image_bytes))\n",
    "            processed = self.preprocess_image(image)\n",
    "            text = pytesseract.image_to_string(processed)\n",
    "            return text.strip()\n",
    "        except Exception as e:\n",
    "            return f\"Error extracting text: {str(e)}\"\n",
    "\n",
    "# Initialize OCR\n",
    "ocr = ImageOCR()\n",
    "\n",
    "# Example usage (uncomment with actual image path)\n",
    "# extracted_text = ocr.extract_text('path/to/job_posting_screenshot.png')\n",
    "# print(\"Extracted Text:\")\n",
    "# print(extracted_text)\n",
    "\n",
    "print(\"âœ… OCR module ready!\")\n",
    "print(\"Note: Install Tesseract OCR on your system for image processing.\")\n",
    "print(\"Ubuntu: sudo apt-get install tesseract-ocr\")\n",
    "print(\"Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete TrustHire Analyzer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrustHireAnalyzer:\n",
    "    \"\"\"\n",
    "    Complete Job Scam Detection System.\n",
    "    Combines ML classification, NLP feature extraction, and OCR.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, vectorizer, feature_extractor, preprocessor, ocr):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.preprocessor = preprocessor\n",
    "        self.ocr = ocr\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze a job offer text and return detailed results.\n",
    "        \"\"\"\n",
    "        # Preprocess\n",
    "        processed_text = self.preprocessor.preprocess(text)\n",
    "        \n",
    "        # Extract features\n",
    "        custom_features = self.feature_extractor.extract_features(text)\n",
    "        \n",
    "        # Vectorize\n",
    "        tfidf_features = self.vectorizer.transform([processed_text])\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.hstack([tfidf_features.toarray(), \n",
    "                                       np.array(list(custom_features.values())).reshape(1, -1)])\n",
    "        \n",
    "        # Predict\n",
    "        prediction = self.model.predict(combined_features)[0]\n",
    "        probability = self.model.predict_proba(combined_features)[0]\n",
    "        \n",
    "        # Calculate trust score (inverse of scam probability)\n",
    "        scam_probability = probability[1]\n",
    "        trust_score = int((1 - scam_probability) * 100)\n",
    "        \n",
    "        # Determine status\n",
    "        if trust_score >= 80:\n",
    "            status = \"genuine\"\n",
    "            status_label = \"Likely Genuine âœ“\"\n",
    "        elif trust_score >= 50:\n",
    "            status = \"caution\"\n",
    "            status_label = \"Proceed with Caution âš \"\n",
    "        else:\n",
    "            status = \"scam\"\n",
    "            status_label = \"Likely Scam âš \"\n",
    "        \n",
    "        # Get red flags\n",
    "        red_flags = self.feature_extractor.get_red_flags(text)\n",
    "        \n",
    "        # Generate advice\n",
    "        advice = self._generate_advice(status, red_flags)\n",
    "        \n",
    "        return {\n",
    "            'trust_score': trust_score,\n",
    "            'status': status,\n",
    "            'status_label': status_label,\n",
    "            'scam_probability': round(scam_probability * 100, 2),\n",
    "            'red_flags': red_flags,\n",
    "            'feature_analysis': custom_features,\n",
    "            'advice': advice\n",
    "        }\n",
    "    \n",
    "    def analyze_image(self, image_path):\n",
    "        \"\"\"\n",
    "        Analyze an image (screenshot) by extracting text first.\n",
    "        \"\"\"\n",
    "        extracted_text = self.ocr.extract_text(image_path)\n",
    "        \n",
    "        if extracted_text.startswith(\"Error\"):\n",
    "            return {\n",
    "                'error': extracted_text,\n",
    "                'trust_score': 0,\n",
    "                'status': 'error'\n",
    "            }\n",
    "        \n",
    "        result = self.analyze_text(extracted_text)\n",
    "        result['extracted_text'] = extracted_text\n",
    "        return result\n",
    "    \n",
    "    def _generate_advice(self, status, red_flags):\n",
    "        \"\"\"Generate safety advice based on analysis\"\"\"\n",
    "        advice = []\n",
    "        \n",
    "        if status == \"scam\":\n",
    "            advice.append(\"âš ï¸ HIGH ALERT: Do not proceed with this offer\")\n",
    "            advice.append(\"Report this to cyber crime authorities\")\n",
    "        elif status == \"caution\":\n",
    "            advice.append(\"Proceed with caution - verify all details\")\n",
    "        else:\n",
    "            advice.append(\"This offer appears legitimate, but stay vigilant\")\n",
    "        \n",
    "        # Add specific advice based on red flags\n",
    "        if \"Payment or fee requested\" in red_flags:\n",
    "            advice.append(\"Never pay money to get a job\")\n",
    "        if \"Personal/banking information requested\" in red_flags:\n",
    "            advice.append(\"Don't share bank details before official onboarding\")\n",
    "        if \"Unprofessional contact methods\" in red_flags:\n",
    "            advice.append(\"Verify contact through official company channels\")\n",
    "        \n",
    "        advice.append(\"Cross-verify company on official registries\")\n",
    "        advice.append(\"Check LinkedIn for employee reviews\")\n",
    "        \n",
    "        return advice\n",
    "\n",
    "# Create the analyzer instance\n",
    "analyzer = TrustHireAnalyzer(\n",
    "    model=best_model,\n",
    "    vectorizer=tfidf_vectorizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    preprocessor=preprocessor,\n",
    "    ocr=ocr\n",
    ")\n",
    "\n",
    "print(\"âœ… TrustHire Analyzer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test the Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 1: Obvious Scam\n",
    "scam_text = \"\"\"\n",
    "Congratulations! You have been selected for a high-paying work from home job.\n",
    "Salary: $10,000 per week guaranteed!\n",
    "Just pay $500 registration fee to start immediately.\n",
    "Contact us on WhatsApp: +1234567890\n",
    "Send your bank account details for salary deposit.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST CASE 1: SCAM MESSAGE\")\n",
    "print(\"=\"*60)\n",
    "result = analyzer.analyze_text(scam_text)\n",
    "\n",
    "print(f\"\\nðŸ”´ Trust Score: {result['trust_score']}/100\")\n",
    "print(f\"ðŸ“Š Status: {result['status_label']}\")\n",
    "print(f\"ðŸ“ˆ Scam Probability: {result['scam_probability']}%\")\n",
    "print(f\"\\nðŸš© Red Flags Detected:\")\n",
    "for flag in result['red_flags']:\n",
    "    print(f\"   â€¢ {flag}\")\n",
    "print(f\"\\nðŸ’¡ Advice:\")\n",
    "for advice in result['advice']:\n",
    "    print(f\"   â€¢ {advice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 2: Genuine Offer\n",
    "genuine_text = \"\"\"\n",
    "Dear Candidate,\n",
    "\n",
    "We are pleased to extend an offer of employment for the position of Software Engineer \n",
    "at TechCorp Inc. Your annual salary will be $95,000 with full benefits including \n",
    "health insurance and 401(k) matching.\n",
    "\n",
    "Please review the attached offer letter and sign by Friday.\n",
    "Your start date will be March 15, 2024.\n",
    "\n",
    "Report to HR on your first day for onboarding.\n",
    "\n",
    "Best regards,\n",
    "HR Department\n",
    "TechCorp Inc.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST CASE 2: GENUINE MESSAGE\")\n",
    "print(\"=\"*60)\n",
    "result = analyzer.analyze_text(genuine_text)\n",
    "\n",
    "print(f\"\\nðŸŸ¢ Trust Score: {result['trust_score']}/100\")\n",
    "print(f\"ðŸ“Š Status: {result['status_label']}\")\n",
    "print(f\"ðŸ“ˆ Scam Probability: {result['scam_probability']}%\")\n",
    "print(f\"\\nðŸš© Red Flags Detected:\")\n",
    "if result['red_flags']:\n",
    "    for flag in result['red_flags']:\n",
    "        print(f\"   â€¢ {flag}\")\n",
    "else:\n",
    "    print(\"   âœ“ No red flags detected\")\n",
    "print(f\"\\nðŸ’¡ Advice:\")\n",
    "for advice in result['advice']:\n",
    "    print(f\"   â€¢ {advice}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Case 3: Suspicious (Caution)\n",
    "suspicious_text = \"\"\"\n",
    "Hi, we saw your resume online and have an urgent opening!\n",
    "The position is for a remote data entry specialist.\n",
    "Salary is $3000 per month. No experience needed.\n",
    "Reply on WhatsApp for more details.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST CASE 3: SUSPICIOUS MESSAGE\")\n",
    "print(\"=\"*60)\n",
    "result = analyzer.analyze_text(suspicious_text)\n",
    "\n",
    "print(f\"\\nðŸŸ¡ Trust Score: {result['trust_score']}/100\")\n",
    "print(f\"ðŸ“Š Status: {result['status_label']}\")\n",
    "print(f\"ðŸ“ˆ Scam Probability: {result['scam_probability']}%\")\n",
    "print(f\"\\nðŸš© Red Flags Detected:\")\n",
    "for flag in result['red_flags']:\n",
    "    print(f\"   â€¢ {flag}\")\n",
    "print(f\"\\nðŸ’¡ Advice:\")\n",
    "for advice in result['advice']:\n",
    "    print(f\"   â€¢ {advice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all components for deployment\n",
    "import pickle\n",
    "\n",
    "model_package = {\n",
    "    'model': best_model,\n",
    "    'vectorizer': tfidf_vectorizer,\n",
    "    'feature_names': feature_names\n",
    "}\n",
    "\n",
    "# Save as pickle\n",
    "with open('trusthire_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_package, f)\n",
    "\n",
    "# Save individual components using joblib\n",
    "joblib.dump(best_model, 'trusthire_rf_model.joblib')\n",
    "joblib.dump(tfidf_vectorizer, 'trusthire_vectorizer.joblib')\n",
    "\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "print(\"Files created:\")\n",
    "print(\"  - trusthire_model.pkl (complete package)\")\n",
    "print(\"  - trusthire_rf_model.joblib (Random Forest model)\")\n",
    "print(\"  - trusthire_vectorizer.joblib (TF-IDF vectorizer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for scam vs genuine\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Scam word cloud\n",
    "scam_text_combined = ' '.join(df[df['label'] == 1]['processed_text'])\n",
    "wc_scam = WordCloud(width=800, height=400, background_color='white', \n",
    "                    colormap='Reds', max_words=100).generate(scam_text_combined)\n",
    "axes[0].imshow(wc_scam, interpolation='bilinear')\n",
    "axes[0].set_title('Scam Job Postings - Common Words', fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Genuine word cloud\n",
    "genuine_text_combined = ' '.join(df[df['label'] == 0]['processed_text'])\n",
    "wc_genuine = WordCloud(width=800, height=400, background_color='white', \n",
    "                       colormap='Greens', max_words=100).generate(genuine_text_combined)\n",
    "axes[1].imshow(wc_genuine, interpolation='bilinear')\n",
    "axes[1].set_title('Genuine Job Postings - Common Words', fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wordclouds.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. API-Ready Function for Web Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_job_offer(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    API-ready function to analyze a job offer.\n",
    "    Can be used with Flask/FastAPI for web integration.\n",
    "    \n",
    "    Args:\n",
    "        text: The job offer text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis result with trust_score, status, findings, and advice\n",
    "    \"\"\"\n",
    "    result = analyzer.analyze_text(text)\n",
    "    \n",
    "    # Format for API response\n",
    "    return {\n",
    "        'score': result['trust_score'],\n",
    "        'status': result['status'],\n",
    "        'findings': [\n",
    "            {'type': 'danger' if 'Payment' in flag or 'Personal' in flag else 'warning', \n",
    "             'message': flag}\n",
    "            for flag in result['red_flags']\n",
    "        ],\n",
    "        'advice': result['advice']\n",
    "    }\n",
    "\n",
    "# Test API function\n",
    "test_result = analyze_job_offer(\"Pay $100 to get this amazing job opportunity!\")\n",
    "print(\"API Response Format:\")\n",
    "print(json.dumps(test_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements a complete ML pipeline for TrustHire:\n",
    "\n",
    "### Components Built:\n",
    "1. **TextPreprocessor** - Cleans and lemmatizes text\n",
    "2. **ScamFeatureExtractor** - Extracts 13+ scam-specific features\n",
    "3. **Multiple ML Models** - Logistic Regression, Random Forest, SVM, etc.\n",
    "4. **ImageOCR** - Extracts text from screenshots using Tesseract\n",
    "5. **TrustHireAnalyzer** - Complete analysis pipeline\n",
    "\n",
    "### Key Features:\n",
    "- TF-IDF vectorization with bigrams\n",
    "- Custom scam indicator detection\n",
    "- Sentiment analysis\n",
    "- OCR for image processing\n",
    "- Trust score calculation (0-100)\n",
    "- Red flag detection and explanation\n",
    "- Smart safety advice generation\n",
    "\n",
    "### Files Generated:\n",
    "- `trusthire_model.pkl` - Complete model package\n",
    "- `trusthire_rf_model.joblib` - Random Forest model\n",
    "- `trusthire_vectorizer.joblib` - TF-IDF vectorizer\n",
    "- `model_comparison.png` - Model performance chart\n",
    "- `feature_importance.png` - Feature importance visualization\n",
    "- `confusion_matrix.png` - Confusion matrix\n",
    "- `wordclouds.png` - Word cloud comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
